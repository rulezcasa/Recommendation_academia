{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61e38084",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bcrypt\n",
    "from pymongo import MongoClient\n",
    "import ssl\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88843709",
   "metadata": {},
   "source": [
    "## Importing and pre-processing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec821c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('merged_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 0','_id','password','Timestamp'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676eb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable SSL verification\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2cfeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing out stopwords in english\n",
    "x=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db8fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def general_preprocess(to_preprocess_column_name, df_name):\n",
    "    to_preprocess_column_name_processed = []\n",
    "    lemma = WordNetLemmatizer()\n",
    "    \n",
    "    for i in range(len(df_name)):\n",
    "        col = df_name.iloc[i][to_preprocess_column_name]\n",
    "        col = re.sub('[^a-zA-Z]', ' ', col)  # Remove non-alphabetic characters\n",
    "        col = col.lower()  # Convert to lowercase\n",
    "        col = col.split()  # Split into words\n",
    "        col = [lemma.lemmatize(word) for word in col if word not in set(stopwords.words('english'))]  # Lemmatize and remove stopwords\n",
    "        col = ' '.join(col)  # Join words back into a single string\n",
    "        to_preprocess_column_name_processed.append(col)\n",
    "    \n",
    "    return to_preprocess_column_name_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_preprocessed=general_preprocess('Topic',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topic_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c66a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Skills_preprocessed=general_preprocess('Skills',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Skills_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2634a6",
   "metadata": {},
   "source": [
    "## Combining target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pd.DataFrame({\n",
    "    'Skills': Skills_preprocessed,\n",
    "    'Topic': Topic_preprocessed\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58800135",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed['Combined'] = df_preprocessed['Skills'] + ', ' + df_preprocessed['Topic']\n",
    "\n",
    "# Drop the individual columns if you only need the combined column\n",
    "df_combined = df_preprocessed.drop(columns=['Skills', 'Topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdcc52",
   "metadata": {},
   "source": [
    "## BERT - large uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a5ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')  # You can change this to other BERT models\n",
    "model = BertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8836b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Use the mean of token embeddings as the sentence embedding\n",
    "        sentence_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        embeddings.append(sentence_embedding)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938024f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_embeddings(df_combined['Combined'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1342b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90e98f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4105ce6b",
   "metadata": {},
   "source": [
    "## Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d973e4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "# Step 3: Apply Affinity Propagation\n",
    "affinity_propagation = AffinityPropagation(affinity='precomputed', random_state=42)\n",
    "labels = affinity_propagation.fit_predict(cosine_sim_matrix)\n",
    "\n",
    "# Step 4: Create a DataFrame to view results\n",
    "df = pd.DataFrame({'Skillset': train_data, 'Cluster': labels})\n",
    "\n",
    "# Print the DataFrame with clusters\n",
    "print(\"Clustered Skillsets with Affinity Propagation:\")\n",
    "print(df)\n",
    "\n",
    "# Print the number of items in each cluster\n",
    "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
    "print(\"\\nNumber of Items in Each Cluster:\")\n",
    "print(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the number of data in each cluster\n",
    "cluster_counts = df['Cluster'].value_counts().sort_index()\n",
    "print(\"\\nNumber of Items in Each Cluster:\")\n",
    "print(cluster_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37d98e",
   "metadata": {},
   "source": [
    "## Dimensionality reduction and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE to the BERT embeddings\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "reduced_matrix = tsne.fit_transform(embeddings_np)\n",
    "\n",
    "# Create a DataFrame with t-SNE results and cluster labels\n",
    "df_tsne = pd.DataFrame(reduced_matrix, columns=['TSNE1', 'TSNE2'])\n",
    "df_tsne['Cluster'] = labels  # Affinity Propagation cluster labels\n",
    "#df_tsne['Skillset'] = train_data  # Optional: Include original skillset data\n",
    "\n",
    "# Plot the clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='TSNE1', y='TSNE2', hue='Cluster', data=df_tsne, palette='viridis', marker='o', s=100)\n",
    "plt.title('Clusters Visualized with t-SNE')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.savefig('affinity_propagation_clusters.png', dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8e3242",
   "metadata": {},
   "source": [
    "## Manually obsering Skillset similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20dc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = df[df['Cluster'] == 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ce4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca166f04",
   "metadata": {},
   "source": [
    "## Intra-cluster similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_cluster_similarities = []\n",
    "\n",
    "# Get unique cluster labels\n",
    "unique_clusters = np.unique(labels)\n",
    "\n",
    "for cluster in unique_clusters:\n",
    "    # Get the indices of the data points in the current cluster\n",
    "    cluster_indices = np.where(labels == cluster)[0]\n",
    "    \n",
    "    if len(cluster_indices) > 1:  # Only compute if more than one point\n",
    "        # Extract the embeddings for the current cluster\n",
    "        cluster_embeddings = embeddings[cluster_indices]\n",
    "\n",
    "        # Compute the cosine similarity matrix for the current cluster\n",
    "        cluster_sim_matrix = cosine_similarity(cluster_embeddings)\n",
    "        \n",
    "        # Get the number of points in the cluster\n",
    "        num_points = len(cluster_indices)\n",
    "        \n",
    "        # Compute the average similarity within the cluster\n",
    "        # Exclude the diagonal (self-similarity) from the average\n",
    "        avg_similarity = (np.sum(cluster_sim_matrix) - num_points) / (num_points * (num_points - 1))\n",
    "        \n",
    "        intra_cluster_similarities.append(avg_similarity)\n",
    "    else:\n",
    "        # Not enough points to calculate\n",
    "        intra_cluster_similarities.append(np.nan)\n",
    "\n",
    "# Calculate the overall intra-cluster similarity\n",
    "# Filter out NaN values (clusters with only one point)\n",
    "valid_similarities = [sim for sim in intra_cluster_similarities if not np.isnan(sim)]\n",
    "overall_intra_cluster_similarity = np.mean(valid_similarities) if valid_similarities else np.nan\n",
    "\n",
    "# Print the overall intra-cluster similarity\n",
    "print(f\"Overall Intra-Cluster Similarity: {overall_intra_cluster_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf07ff2",
   "metadata": {},
   "source": [
    "## Sillhouette Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273948d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Assuming `embeddings` is your feature matrix and `labels` are your cluster labels\n",
    "silhouette_avg = silhouette_score(embeddings, labels)\n",
    "\n",
    "# Print the Silhouette Score\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af036687",
   "metadata": {},
   "source": [
    "## Davies-Bouldin Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Compute the Davies-Bouldin Index\n",
    "dbi = davies_bouldin_score(embeddings, labels)\n",
    "\n",
    "# Print the Davies-Bouldin Index\n",
    "print(f\"Davies-Bouldin Index: {dbi:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb34e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Combined'] = df['Skillset']\n",
    "data['Cluster']=df['Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e05e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304ce1b",
   "metadata": {},
   "source": [
    "## Recommendation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc692fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommend_profiles_by_topic_skills(user_index, df, top_n=5):\n",
    "    \"\"\"\n",
    "    Recommends the top N profiles to a target profile based on 'Topic' and 'Skills' columns.\n",
    "\n",
    "    Parameters:\n",
    "    - user_index (int): The index of the target profile in the DataFrame.\n",
    "    - df (pandas.DataFrame): The DataFrame containing profile data.\n",
    "    - top_n (int): The number of top profiles to recommend.\n",
    "\n",
    "    Returns:\n",
    "    - list of tuples: Each tuple contains the index and similarity score of a recommended profile.\n",
    "    \"\"\"\n",
    "    \n",
    "    bert_embeddings = get_embeddings(data['Combined']) \n",
    "    bert_embeddings = np.array(bert_embeddings)\n",
    "    \n",
    "    # Get the TF-IDF vector for the target profile\n",
    "    target_vector = bert_embeddings[user_index].reshape(1, -1)\n",
    "    \n",
    "    # Compute cosine similarities between the target profile and all other profiles\n",
    "    similarities = cosine_similarity(target_vector, bert_embeddings).flatten()\n",
    "    \n",
    "    # Exclude the target profile itself by setting its similarity score to -1\n",
    "    similarities[user_index] = -1\n",
    "    \n",
    "    # Get the indices of the top N most similar profiles\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # Create a list of tuples with index and similarity score\n",
    "    recommendations = [(idx, similarities[idx]) for idx in top_indices]\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def print_profile_details(profile_index, df):\n",
    "    \"\"\"\n",
    "    Prints the details of a specific profile.\n",
    "\n",
    "    Parameters:\n",
    "    - profile_index (int): The index of the profile in the DataFrame.\n",
    "    - df (pandas.DataFrame): The DataFrame containing profile data.\n",
    "    \"\"\"\n",
    "    profile_details = df.iloc[profile_index]\n",
    "    print(f\"Details of Target Profile (Index: {profile_index}):\")\n",
    "    print(f\"Name: {profile_details['Name']}\")\n",
    "    print(f\"Email: {profile_details['Email']}\")\n",
    "    print(f\"Skills: {profile_details['Skills']}\")\n",
    "    print(f\"Domain Interest: {profile_details['Topic']}\")\n",
    "    print(f\"Cluster: {profile_details['Cluster']}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "\n",
    "def print_recommendation_details(recommendations, df):\n",
    "    \"\"\"\n",
    "    Prints the details of the recommended profiles.\n",
    "\n",
    "    Parameters:\n",
    "    - recommendations (list of tuples): Each tuple contains the index and similarity score of a recommended profile.\n",
    "    - df (pandas.DataFrame): The DataFrame containing profile data.\n",
    "    \"\"\"\n",
    "    for idx, score in recommendations:\n",
    "        profile_details = data.iloc[idx]\n",
    "        print(f\"Index: {idx}\")\n",
    "        print(f\"Similarity Score: {score:.4f}\")\n",
    "        print(f\"Name: {profile_details['Name']}\")\n",
    "        print(f\"Email: {profile_details['Email']}\")\n",
    "        print(f\"Skills: {profile_details['Skills']}\")\n",
    "        print(f\"Domain Interest: {profile_details['Topic']}\")\n",
    "        print(f\"Cluster: {profile_details['Cluster']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "target_profile_index = 575  # Replace with the index of the target profile\n",
    "print_profile_details(target_profile_index, data)\n",
    "top_n_recommendations = recommend_profiles_by_topic_skills(target_profile_index, data, top_n=5)\n",
    "print_recommendation_details(top_n_recommendations, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def generate_keyword_cloud(df, column, title):\n",
    "    \"\"\"\n",
    "    Generates and plots a word cloud for a specific column in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The DataFrame containing profile data.\n",
    "    - column (str): The column name for which the word cloud is to be generated.\n",
    "    - title (str): The title for the word cloud plot.\n",
    "    \"\"\"\n",
    "    # Combine all text in the specified column\n",
    "    text = \" \".join(df[column].dropna().astype(str).values)\n",
    "    \n",
    "    # Generate the word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    \n",
    "    # Plot the word cloud\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('keyword_cloud.png', dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Generate and plot the keyword cloud for 'Skills'\n",
    "generate_keyword_cloud(data, 'Skills', 'Keyword Cloud for recommendations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe85140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
